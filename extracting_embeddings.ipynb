{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "falling-neutral",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |███████████▋                    | 321.5 MB 123.1 MB/s eta 0:00:05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████        | 660.0 MB 134.1 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 881.9 MB 5.2 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 100.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/default/lib/python3.8/site-packages (from torch) (3.10.0.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2021.11.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[K     |████████████████████████████████| 764 kB 78.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/default/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 1.5 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/default/lib/python3.8/site-packages (from transformers) (4.61.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 82.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/default/lib/python3.8/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in /home/default/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/default/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/default/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/default/lib/python3.8/site-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/default/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/default/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/default/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in /home/default/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in /home/default/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/default/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: regex, filelock, tokenizers, sacremoses, huggingface-hub, transformers, torch\n",
      "Successfully installed filelock-3.4.2 huggingface-hub-0.2.1 regex-2021.11.10 sacremoses-0.0.46 tokenizers-0.10.3 torch-1.10.1 transformers-4.15.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/default/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beginning-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "existing-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast\n",
    "import torch\n",
    "\n",
    "model = ElectraForPreTraining.from_pretrained(\"Seznam/small-e-czech\", output_hidden_states = True)\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"Seznam/small-e-czech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "convenient-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ufal/robeczech-base\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"ufal/robeczech-base\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "chinese-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask, last_hidden_layers):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    token_embeddings = torch.stack(model_output[1][-last_hidden_layers:]).mean(axis=0)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def sum_pooling(model_output, attention_mask, last_hidden_layers):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    token_embeddings = torch.stack(model_output[1][-last_hidden_layers:]).sum(axis=0)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def concat_pooling(model_output, attention_mask, last_hidden_layers, inputs):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    token_embeddings = torch.stack(model_output[1][-last_hidden_layers:]).reshape(inputs,30,-1)\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cutting-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = pd.read_csv(\"in/tables/MATERIALS_FOR_CLASSIFICATION_CLEAN.csv\")\n",
    "cats = pd.read_csv(\"in/tables/MATERIAL_CATEGORY_FOR_CLASSIFICATION.csv\")\n",
    "\n",
    "df = pd.merge(mats,cats,on=[\"MATERIAL_ODS_ID\",\"SHOP\"])\n",
    "\n",
    "def create_category_dict(df):\n",
    "    return df.groupby(\"CATEGORY_ID\").CATEGORY_PATH.unique().to_dict()\n",
    "category_dict = create_category_dict(df)\n",
    "\n",
    "# should be applied after category_decoder_dict\n",
    "df = df[~df.MATERIAL_ID.duplicated()].reset_index(drop=True).copy()\n",
    "\n",
    "for k,v in category_dict.items():\n",
    "    df.loc[df.CATEGORY_ID == k,\"CATEGORY_PATH_FIX\"] = v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "brilliant-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df.CATEGORY_PATH.str.contains(\"Sekačky\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "swiss-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NAME_BRIEF = df.NAME_BRIEF.str.split(\" ; \").apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "direct-diary",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-d69b7ead06bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Tokenize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     encoded_input = tokenizer(df_subset['NAME_BRIEF'].tolist(), max_length=10,\n\u001b[0m\u001b[1;32m      7\u001b[0m                               padding=True, truncation=True, return_tensors='pt')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 )\n\u001b[1;32m   2417\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2419\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2607\u001b[0m         )\n\u001b[1;32m   2608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2609\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2610\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2611\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/default/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# we add an overflow_to_sample_mapping array (see below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0msanitized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_encodings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(0,df.shape[0]+1000,1000,):\n",
    "    \n",
    "    df_subset = df.iloc[i:i+1000][[\"MATERIAL_ID\",\"NAME_BRIEF\"]].copy()\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(df_subset['NAME_BRIEF'].tolist(), max_length=10,\n",
    "                              padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    mean_1 = mean_pooling(model_output, encoded_input['attention_mask'],1)\n",
    "    sum_4 = sum_pooling(model_output, encoded_input['attention_mask'],4)\n",
    "#     concat_4 = concat_pooling(model_output, encoded_input['attention_mask'],4,inputs=1000)\n",
    "\n",
    "    df_subset[\"MEAN_1\"] = [list(np.round(i,4)) for i in mean_1.tolist()]\n",
    "    df_subset[\"SUM_4\"] = [list(np.round(i,4)) for i in sum_4.tolist()]\n",
    "#     df_subset[\"CONCAT_4\"] = [list(np.round(i,4)) for i in concat_4.tolist()]\n",
    "    \n",
    "    with open(\"out/tables/SENT_EMBD_ROBERTA.csv\", \"a\") as f:\n",
    "                    df_subset[[\"MATERIAL_ID\",\"MEAN_1\",\"SUM_4\"]].to_csv(f, index=False, header=True)\n",
    "    del encoded_input, model_output, mean_1, sum_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-uncle",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "flying-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similiarity = cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "forty-mortgage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sekačka motorová WORLD W46P'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4].NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "prescription-cincinnati",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sekačka motorová WORLD W46P', 'Sekačka motorová WORLD W45P',\n",
       "       'Sekačka motorová WORLD W53 AL PRS',\n",
       "       'Sekačka motorová WORLD W55 4S', 'Sekačka motorová WORLD W50P RS',\n",
       "       'Sekačka motorová WORLD W50P IS', 'Sekačka motorová WORLD W46P IS',\n",
       "       'Ryobi RLT3525 - 350W Elektrická strunová sekačka, šířka záběru 25cm',\n",
       "       'elektrická sekačka ARM 34 06008A6101',\n",
       "       'elektrická sekačka ARM 37 06008A6201',\n",
       "       'AL5 51 SHQ benzínová sekačka s pojezdem',\n",
       "       'akumulátor High Power 36 V - 4,0 Ah',\n",
       "       ' AL5 51 SAQ benzínová sekačka s pojezdem', 'GH-PM 40P',\n",
       "       '752 SXH DOV 5in1',\n",
       "       'Strunová sekačka 18 V LTR 18-30 Battery (1.444-310.0) - zánovní',\n",
       "       'Sekačka strunová akumulátorová GT 2025 (bez aku) - zánovní',\n",
       "       'Strunová sekačka 18 V LTR 18-30 Battery (1.444-310.0) - rozbaleno',\n",
       "       'Ryobi RLT5127 - 500W Elektrická strunová sekačka, šířka záběru 27cm',\n",
       "       'Kartáče na kola SILENO City (4030-20)', 'BC430PRO', 'OPTIMA 38 E',\n",
       "       'Ryobi RY18LM37A-240 - 18V Akumulátorová sekačka, šířka záběru 37cm (2x 4.0Ah)',\n",
       "       '548 SWE 5 in 1', 'VE24250 - zánovní',\n",
       "       'Motorový olej 4T SAE 30, 1 l',\n",
       "       'Sekačka elektrická GC-EM 1032 3400257',\n",
       "       'Motorový olej 4T SAE 30, 600 ml',\n",
       "       'Motorový olej 4T SAE 30, 1,4 l', 'LM 530/36 Bp'], dtype=object)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[np.argsort(cosine_similiarity[4])[::-1][:30]].NAME.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
